{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_classifier_Ngram_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/diego-carvalho/tweet-classifier/blob/master/Tweet_classifier_Ngram_model.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fuYEPjI8tThK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Para baixar os dados e executar:\n",
        "\n",
        "Copie esse arquivo para seu google Drive:  https://drive.google.com/open?id=1cEAp9cCI7Z-FZL00gIzDcz1Lq2Agxotp\n",
        "\n",
        "troque o `file_id` na linha 17 pelo id que aparece no link compartilhável do Google Drive.\n",
        "\n",
        "para executar uma célula, use `shift-enter` (executa e vai para a próxima célula) ou `ctrl-enter`(só executa a célula)\n"
      ]
    },
    {
      "metadata": {
        "id": "EER613u4qWst",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "\n",
        "def clean_tweet(x):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  return re.sub(r'\\d+', '', \" \".join(tokenizer.tokenize(x)).lower())\n",
        "  \n",
        "\n",
        "\n",
        "def load_dataset(seed=42, test_size=0.2, binary=False):\n",
        "  \"\"\"Baixa o dataset e separa em treino e teste\n",
        "  Params:\n",
        "    seed: Seed para separação aleatória\n",
        "    test_size: Percentual dos dados para teste\n",
        "    binary: Se a avaliação vai ser binária (discurso de ódio ou não) \n",
        "            ou de três classes: Discurso de ódio, linguagem ofensivo ou nada\n",
        "  \n",
        "  Return:\n",
        "    train(test)_tweets: Tweets para treino/test do algoritmo (pd.series de strings)\n",
        "    train(test)_labels: categorização dos tweets (array de integers)\n",
        "    \n",
        "  \"\"\"\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  file_id = \"1nHhy8X2MN85qvBUR4PlJZS8j9j-gwp61\"\n",
        "  downloaded  = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile('labeled_data.p')\n",
        "  \n",
        "  data_set = pd.read_pickle(\"labeled_data.p\")\n",
        "  if binary:\n",
        "    data_set[\"class\"] = data_set[\"class\"].apply(lambda x: 1 if x==1 else 0)\n",
        "  train, test = train_test_split(data_set, test_size=test_size, shuffle=True)\n",
        "  train_tweets = train.tweet.apply(clean_tweet)\n",
        "  train_labels = train[\"class\"]\n",
        "  test_tweets = test.tweet.apply(clean_tweet)\n",
        "  test_labels = test[\"class\"]  \n",
        "  return ((train_tweets, np.array(train_labels)), \n",
        "          (test_tweets, np.array(test_labels)))\n",
        "(train, train_labels), (test, test_labels) = load_dataset(binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Fc_EDvMqZ65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tokenization + vetorization + Tf-IDF encoding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "NGRAM_RANGE = (1,2)\n",
        "TOP_K = 20000\n",
        "TOKEN_MODE = 'word'\n",
        "MIN_DOCUMENT_FREQUENCY = 29\n",
        "\n",
        "def ngram_vectorizer(train, train_labels, test):\n",
        "  vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE,\n",
        "                               strip_accents='unicode', decode_error='replace',\n",
        "                               analyzer=TOKEN_MODE, \n",
        "                               min_df=MIN_DOCUMENT_FREQUENCY)\n",
        "  \"\"\"Gera vetor de TF-IDF com ngrams de tweets\n",
        "  Params:\n",
        "    ngram_range: tupla com o range de ngrams (1,2), por exemplo\n",
        "    strip_accents: Estratégia para retirar acentos\n",
        "    decode_error: Estratégia para lidar com erros\n",
        "    analyzer: Modo de análise (palavra ou char?)\n",
        "    min_df: Mínimo de documentos que devem conter o termo para ele ser considerado\n",
        "    \"\"\"\n",
        "\n",
        "  X_train = vectorizer.fit_transform(train)\n",
        "  X_test = vectorizer.transform(test)\n",
        "\n",
        "  selector = SelectKBest(f_classif, k=min(TOP_K, X_train.shape[1]))\n",
        "  selector.fit(X_train, train_labels)\n",
        "  X_train = selector.transform(X_train).astype('float32')\n",
        "  X_test = selector.transform(X_test).astype('float32')\n",
        "  return X_train, X_test, vectorizer, selector\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvwBk2QOrOaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "\n",
        "def mlp_model(layers, units, input_shape, num_classes,  dropout_rate=0.0):\n",
        "  \n",
        "  \"\"\"Define o modelo de multi-layer perceptron a ser utilizado\n",
        "  Params:\n",
        "    layers: Número de camadas\n",
        "    units: Número de neuronios por camada\n",
        "    input_shape: Tamanho da entrada\n",
        "    num_classes: Quantas classes devem ser classificadas\n",
        "    droput_rate: Probabilide de um neurônio ser ignorado. \n",
        "                 Serve para evitar overfitting\n",
        "    \"\"\"\n",
        "  output_units = num_classes\n",
        "  if output_units==2:\n",
        "    output_units=1\n",
        "    output_activation=\"sigmoid\"\n",
        "  else:\n",
        "    output_activation = \"softmax\"\n",
        "  print(output_activation)\n",
        "  model = models.Sequential()\n",
        "  model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "  \n",
        "  for i in range(layers-1):\n",
        "    model.add(Dense(units = units[i], activation='relu',\n",
        "                    kernel_initializer=\"lecun_uniform\",\n",
        "                    bias_initializer=\"zeros\"))\n",
        "    \n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "  model.add(Dense(units=output_units, activation=output_activation,\n",
        "                  kernel_initializer=\"lecun_uniform\",\n",
        "                  bias_initializer=\"zeros\"))\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSmEfQUn3Lck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def train_ngram_model(data,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.0\n",
        "                     ):\n",
        "  \"\"\"Treina um modelo de ponta-a-ponta\n",
        "  Params:\n",
        "    data: Tupla de tuplas: ((treino, label_treino), (teste, label_teste))\n",
        "    learning_rate: O quão rápido o algoritmo vai aprender.\n",
        "                   Quanto menor, melhor, mas mais demorado.\n",
        "    epochs: Máximo de \"passadas\" pelos dados\n",
        "    batch_size: Quantos exemplos são usados por vez para aprender\n",
        "    layers: Número de camadas\n",
        "    units: Número de neuronios por camada\n",
        "    droput_rate: Probabilide de um neurônio ser ignorado. \n",
        "                 Serve para evitar overfitting                   \n",
        "    \"\"\"\n",
        "  \n",
        "  (X_train, train_labels), (X_test, test_labels) = data\n",
        "  X_train, X_test, vectorizer, selector = ngram_vectorizer(X_train, train_labels, X_test)\n",
        "  num_classes = len(np.unique(train_labels))\n",
        "\n",
        "  model = mlp_model(layers=layers, units=units, dropout_rate=dropout_rate,\n",
        "                  input_shape=X_train.shape[1:], num_classes=num_classes)\n",
        "\n",
        "  if num_classes==2:\n",
        "    loss=\"binary_crossentropy\"\n",
        "  else:\n",
        "    loss=\"sparse_categorical_crossentropy\"\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[\"acc\"])\n",
        "\n",
        "  callbacks= [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
        "  history = model.fit(X_train, train_labels,\n",
        "                      epochs=epochs, callbacks=callbacks,\n",
        "                      validation_data=(X_test, test_labels),\n",
        "                      verbose=1, batch_size=batch_size)\n",
        "  history = history.history\n",
        "  print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "      acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "  model.save(\"hate_sppech_model.md5\")\n",
        "  return model, vectorizer, selector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtV4P_oiwTci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "46fd29e0-fd79-4339-c5f6-51cc9535d52b"
      },
      "cell_type": "code",
      "source": [
        "model, vectorizer, selector = train_ngram_model(((train, train_labels), (test, test_labels)), layers=3, units=(256, 128, 64))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid\n",
            "Train on 19826 samples, validate on 4957 samples\n",
            "Epoch 1/1000\n",
            "19826/19826 [==============================] - 2s 119us/step - loss: 0.3989 - acc: 0.8190 - val_loss: 0.2869 - val_acc: 0.8874\n",
            "Epoch 2/1000\n",
            "19826/19826 [==============================] - 1s 59us/step - loss: 0.2506 - acc: 0.9022 - val_loss: 0.2742 - val_acc: 0.8925\n",
            "Epoch 3/1000\n",
            "19826/19826 [==============================] - 1s 60us/step - loss: 0.2133 - acc: 0.9201 - val_loss: 0.2734 - val_acc: 0.8957\n",
            "Epoch 4/1000\n",
            "19826/19826 [==============================] - 1s 60us/step - loss: 0.1695 - acc: 0.9403 - val_loss: 0.2883 - val_acc: 0.8951\n",
            "Epoch 5/1000\n",
            "19826/19826 [==============================] - 1s 61us/step - loss: 0.1186 - acc: 0.9594 - val_loss: 0.3254 - val_acc: 0.8901\n",
            "Validation accuracy: 0.8900544680797792, loss: 0.32540685554377513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "znS3fZaGByjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a30a1e85-95b7-4313-d9be-84decce2e0f0"
      },
      "cell_type": "code",
      "source": [
        "sample_tweet = \"Someone in SF just honked and yelled \\\"learn to fucking drive.\\\" It was a driveless car\"\n",
        "# sample_tweet = \"fucking hispanics stealing our jobs\"\n",
        "transformed_tweet = selector.transform(vectorizer.transform([clean_tweet(sample_tweet)]))\n",
        "model.predict(transformed_tweet)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91333956]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "MYY44mQeKK9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}